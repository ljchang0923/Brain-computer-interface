{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17114,"status":"ok","timestamp":1653290610254,"user":{"displayName":"張力仁","userId":"06680874629741480260"},"user_tz":-480},"id":"S-rwaqxFL4L5","outputId":"cb4890e7-eb1a-4183-8147-dc4c448a03b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56fqPpdoMDg5"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import init\n","from torch.utils.data import TensorDataset, DataLoader\n","import random\n","from scipy import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","BATCH_SIZE = 128\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbOj-5GYMHxj"},"outputs":[],"source":["def get_device():\n","    return 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CScSKniWMJ83"},"outputs":[],"source":["def choose_scheme(path, scheme):\n","  filedir = os.listdir(path)\n","  x_train, x_test, y_train, y_test = [], [], [], []\n","  x_fine_tune, y_fine_tune = [], []\n","  # trial_class0, trial_class1, \n","  # 將第一個subject 1固定作為testing subject\n","  subject_t = \"BCIC_S01_T.mat\"\n","  subject_e = \"BCIC_S01_E.mat\"\n","  x_train = torch.Tensor(io.loadmat(os.path.join(path, subject_t))['x_train']).unsqueeze(1)\n","  y_train = torch.Tensor(io.loadmat(os.path.join(path, subject_t))['y_train']).view(-1).long()\n","  x_test = torch.Tensor(io.loadmat(os.path.join(path, subject_e))['x_test']).unsqueeze(1)\n","  y_test = torch.Tensor(io.loadmat(os.path.join(path, subject_e))['y_test']).view(-1).long()\n","  len_x = x_train.size()[0]\n","\n","  # 依序將其他subject的data讀入\n","  for filename in filedir:\n","    #print(filename)\n","    if filename in {subject_t, subject_e}:\n","      continue\n","    elif filename.endswith('E.mat'):\n","      x = torch.Tensor(io.loadmat(os.path.join(path, filename))['x_test']).unsqueeze(1)\n","      y = torch.Tensor(io.loadmat(os.path.join(path, filename))['y_test']).view(-1).long()\n","      x_train = torch.cat([x_train, x])\n","      y_train = torch.cat([y_train, y])\n","    elif filename.endswith('T.mat'):\n","      x = torch.Tensor(io.loadmat(os.path.join(path, filename))['x_train']).unsqueeze(1)\n","      y = torch.Tensor(io.loadmat(os.path.join(path, filename))['y_train']).view(-1).long()\n","      x_train = torch.cat([x_train, x])\n","      y_train = torch.cat([y_train, y])\n","\n","  # choose real training and testing data based on scheme\n","  if scheme == 'individual': # indiviual will access only training session from subject1\n","    return [x_train[:len_x], y_train[:len_x], x_test, y_test]\n","  elif scheme == 'dependent': # dependent scheme collect all data except for the testing sessions from sub1\n","    return [x_train, y_train, x_test, y_test]\n","  elif scheme == 'independent': # independent scheme collect all data except for testing and training session from sub1\n","    return [x_train[len_x:], y_train[len_x:], x_test, y_test]\n","  elif scheme == 'fine-tune': # the data used on fine tune is the same as individual scheme\n","    return [x_train[:len_x], y_train[:len_x], x_test, y_test]\n","  else:\n","    raise ValueError('unexpected scheme, enter other scheme again')\n","  \n","def get_dataloader(data_path, scheme):\n","  data = []\n","  data = choose_scheme(data_path, scheme)\n","\n","  x_train, y_train, x_test, y_test = data\n","\n","  print(\"x_train shape: \", x_train.size())\n","  print(\"y_train shape: \", y_train.size())\n","  print(\"x_test shape: \", x_test.size())\n","  print(\"y_test shape: \", y_test.size())\n","\n","  # 存成tensordataset\n","  train_dataset = TensorDataset(x_train, y_train)\n","  test_dataset = TensorDataset(x_test, y_test)\n","  # 包成dataloader\n","  train_dl = DataLoader(\n","      dataset = train_dataset,\n","      batch_size = BATCH_SIZE,\n","      shuffle = True,\n","      num_workers = 0   \n","  )\n","  test_dl = DataLoader(\n","      dataset = test_dataset,\n","      batch_size = len(test_dataset),\n","      shuffle = False,\n","      num_workers = 0   \n","  )\n","  return [train_dl, test_dl]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjCkRKdHMoIa"},"outputs":[],"source":["## 4 CNN model\n","class basic_CNN(nn.Module):\n","    def __init__(self, filter_size):\n","        super(basic_CNN, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(1, 50, (1, filter_size), padding=(0, 25), bias=False),\n","            nn.BatchNorm2d(50)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(50, 50, (22, 1), groups=50, bias=False),\n","            nn.BatchNorm2d(50),\n","            nn.ELU(),\n","            nn.MaxPool2d((1, 3), stride=3),\n","            nn.Dropout(0.5)\n","        )\n","    def forward(self, input):\n","        x = self.conv1(input)\n","        x = self.conv2(x) #shape of x:(batch,50,1,196)\n","        return x\n","\n","class CNN1(nn.Module):\n","    def __init__(self):\n","        super(CNN1, self).__init__()\n","        self.basic_cnn = basic_CNN(30)\n","        self.fc = nn.Linear(5000,1024) # sliding:5000 normal:9700 \n","        self.classifier = nn.Linear(1024, 4)\n","\n","    def forward(self, input):\n","        x = self.basic_cnn(input)\n","        # print(f\"latent size: {x.size()}\")\n","        x = torch.flatten(x,1)\n","        latent = self.fc(x)\n","        output = self.classifier(latent)\n","\n","        return latent, output\n","\n","    \n","class CNN2(nn.Module):\n","    def __init__(self):\n","        super(CNN2, self).__init__()\n","        self.basic_cnn = basic_CNN(25)\n","        self.conv_block = nn.Sequential(\n","            nn.Conv2d(50, 100, (1,10), padding=(0,5), bias=False),\n","            nn.BatchNorm2d(100),\n","            nn.ELU(),\n","            nn.MaxPool2d((1,3), stride=3),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc = nn.Linear(6500,1024)\n","        self.classifier = nn.Linear(1024, 4)\n","\n","    def forward(self, input):\n","        x = self.basic_cnn(input)\n","        x = self.conv_block(x)\n","        # print(f\"latent size: {x.size()}\")\n","        x = torch.flatten(x,1)\n","        latent = self.fc(x)\n","        output = self.classifier(latent)\n","\n","        return latent, output\n","\n","class CNN3(nn.Module):\n","    def __init__(self):\n","        super(CNN3, self).__init__()\n","        self.basic_cnn = basic_CNN(20)\n","        self.conv_block1 = nn.Sequential(\n","            nn.Conv2d(50, 100, (1,10), padding=(0,5), bias=False),\n","            nn.BatchNorm2d(100),\n","            nn.ELU(),\n","            nn.MaxPool2d((1,3), stride=3),\n","            nn.Dropout(0.5)\n","        )\n","        self.conv_block2 = nn.Sequential(\n","            nn.Conv2d(100, 100, (1,10), padding=(0,5), bias=False),\n","            nn.BatchNorm2d(100),\n","            nn.ELU(),\n","            nn.MaxPool2d((1,3), stride=3),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc = nn.Linear(2200, 1024)\n","        self.classifier = nn.Linear(1024, 4)\n","\n","    def forward(self, input):\n","        x = self.basic_cnn(input)\n","        x = self.conv_block1(x)\n","        x = self.conv_block2(x)\n","        # print(f\"latent size: {x.size()}\")\n","        x = torch.flatten(x,1)\n","        latent = self.fc(x)\n","        output = self.classifier(latent)\n","        return latent, output\n","\n","class CNN4(nn.Module):\n","    def __init__(self):\n","        super(CNN4, self).__init__()\n","        self.basic_cnn = basic_CNN(10)\n","        self.conv_block1 = nn.Sequential(\n","            nn.Conv2d(50, 100, (1,10), padding=(0,5), bias=False),\n","            nn.BatchNorm2d(100),\n","            nn.ELU(),\n","            nn.MaxPool2d((1,3), stride=3),\n","            nn.Dropout(0.5)\n","        )\n","        self.conv_block2 = nn.Sequential(\n","            nn.Conv2d(100, 100, (1,10), padding=(0,5), bias=False),\n","            nn.BatchNorm2d(100),\n","            nn.ELU(),\n","            nn.MaxPool2d((1,3), stride=3),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc = nn.Linear(700, 1024)\n","        self.classifier = nn.Linear(1024, 4)\n","        \n","    def forward(self, input):\n","        x = self.basic_cnn(input)\n","        x = self.conv_block1(x)\n","        x = self.conv_block2(x)\n","        x = self.conv_block2(x)\n","        # print(f\"latent size: {x.size()}\")\n","        x = torch.flatten(x,1)\n","        latent = self.fc(x)\n","        output = self.classifier(latent)\n","        return latent, output\n","\n","class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(4096,50)\n","        self.drop1 = nn.Dropout(0.5)\n","        self.elu = nn.ELU()\n","        self.fc2 = nn.Linear(50,50)\n","        self.drop2 = nn.Dropout(0.5)\n","        #self.softmax = nn.LogSoftmax(dim=1)\n","        self.classifier = nn.Linear(50,4)\n","\n","        self.layers = nn.Sequential(self.fc1,self.drop1,self.elu,self.fc2,self.drop2,self.classifier)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        init.xavier_uniform_(self.fc1.weight)\n","        init.xavier_uniform_(self.fc2.weight)\n","\n","    def forward(self, input):\n","        '''x = self.fc1(input)\n","        x = self.fc2(x)\n","        output = self.classifier(x)'''\n","        output = self.layers(input)\n","        dummy = 0\n","        return dummy, output\n","\n","class AE(nn.Module):\n","    def __init__(self):\n","        super(AE, self).__init__()\n","        self.encode = nn.Linear(4096,100)\n","        self.decode = nn.Linear(100,4096)\n","        self.classifier = nn.Linear(4096,4)\n","\n","    def forward(self, input):\n","        x = self.encode(input)\n","        output = self.decode(x)\n","        with torch.no_grad():\n","            pred = self.classifier(output)\n","\n","        return pred, output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCwmJOb9MXtV"},"outputs":[],"source":["def train_model(model, train_dl, test_dl, device, config):\n","  # set the optimizer and corresponding loss function\n","  optimizer = getattr(optim, config['optimizer'])(model.parameters(), lr=config['lr'], weight_decay=0.0001)\n","  criterion = nn.CrossEntropyLoss()\n","  #lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n","  record = {\n","      'train_loss': [],\n","      'train_acc': [],\n","      'val_loss': [],\n","      'val_acc': []\n","      }\n","  \n","  max_acc = 0\n","  print('\\n training start')\n","  # start training process\n","  for epoch in range(config['epoch']):\n","    model.train()\n","    train_loss = 0\n","    train_acc = 0\n","    for x_train, y_train in train_dl:\n","      x_train, y_train = x_train.to(device), y_train.to(device)\n","      optimizer.zero_grad()\n","      _, out = model(x_train) # model prediction\n","      loss = criterion(out, y_train) # calculate loss\n","      pred = torch.argmax(out, axis = 1) # max out the prediction\n","      train_loss += loss.detach().cpu().item()\n","      train_acc += (pred==y_train).sum().item() # count the corrected classified casese\n","\n","      loss.backward() # derive backpropagation\n","      optimizer.step() # update model\n","    \n","    # save the model which produce maximum validation accuracy\n","    val_acc = val_model(model, test_dl, device)\n","    if(val_acc>max_acc):\n","      max_acc = val_acc\n","      torch.save(model.state_dict(), f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")\n","\n","    record['train_loss'].append(train_loss/len(train_dl))\n","    record['train_acc'].append(train_acc/len(train_dl.dataset))\n","    record['val_acc'].append(val_acc)\n","      \n","    #lr_scheduler.step()\n","    if (epoch+1)%10 == 0:\n","      print(f'epoch: {epoch+1}, training acc: {train_acc/len(train_dl.dataset)} val_acc: {val_acc}')\n","  \n","  return record\n","\n","def val_model(model, test_dl, device):\n","  model.eval()\n","  # print('test start')\n","  acc = 0\n","  with torch.no_grad():\n","    for x_test, y_test in test_dl:\n","      x_test, y_test = x_test.to(device), y_test.to(device)\n","      _, pred = model(x_test)\n","      pred = torch.argmax(pred, axis = 1)\n","      acc += (pred==y_test).sum().item()\n","  acc /= len(test_dl.dataset)\n"," \n","  return acc\n","\n","def test_model(model, test_dl, device):\n","  model.eval()\n","  # print('test start')\n","  acc = 0\n","  latent = []\n","  output = []\n","  with torch.no_grad():\n","    for x_test, y_test in test_dl:\n","      x_test, y_test = x_test.to(device), y_test.to(device)\n","      vector, pred = model(x_test)\n","      pred = torch.argmax(pred, axis = 1)\n","      output.append(pred)\n","      latent.append(vector)\n","      acc += (pred==y_test).sum().item()\n","  acc /= len(test_dl.dataset)\n","  print(f\"testing accuracy: {acc*100}%\")\n"," \n","  return latent, output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qh-2sj6JMZGq"},"outputs":[],"source":["def main():\n","  device = get_device()\n","  set_seed(33)\n","  file_dir = '/content/gdrive/Shareddrives/BCI/BCI_data'\n","  scheme = 'independent' # individual, dependent, independent, fine-tune\n","  dl = get_dataloader(file_dir, scheme)\n","\n","  if scheme == 'fine-tune':\n","    tune_dl, test_dl, _ = dl  \n","    config = {\n","        'epoch': 300,\n","        'optimizer': 'Adam',\n","        'lr': 0.001,\n","        'model': 'CNN1',\n","        'scheme': scheme,\n","        'save_path': '/content/gdrive/Shareddrives/BCI/CNN_model/'\n","    }\n","    # os.makedirs('/content/gdrive/MyDrive/model', exist_ok=True)\n","    model = CNN1().to(device) # CNN1, CNN2, CNN3, CNN4\n","    model.load_state_dict(torch.load('/content/gdrive/Shareddrives/BCI/CNN_model/CNN1_independent_0.001_128')) # 要改load model的檔名\n","    #for name, param in model.named_parameters():\n","    #  if name != 'classifier.weight' and name != 'classifier.bias':\n","    #    param.requires_grad = False\n","    loss_record = train_model(model, tune_dl, test_dl, device, config)\n","  else:\n","    train_dl, test_dl, _ = dl\n","    model = CNN1().to(device)  # CNN1, CNN2, CNN3, CNN4\n","    config = {\n","        'epoch': 500,\n","        'optimizer': 'Adam',\n","        'lr': 0.0001,\n","        'scheme': scheme,\n","        'model': 'CNN1',\n","        'save_path': '/content/gdrive/Shareddrives/BCI/CNN_model/sliding/'\n","    }\n","    # os.makedirs('/content/gdrive/MyDrive/model', exist_ok=True)\n","    loss_record = train_model(model, train_dl, test_dl, device, config)\n","\n","  # plot confusion matrix\n","#   model = EEGNet().to(device)\n","#   model.load_state_dict(torch.load(f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")) # 要改load model的檔名\n","#   plot_confusion_matrix(model, test_dl, device)\n","  \n","def plot_confusion_matrix(model, test_dl, device):\n","  pred = test_model(model, test_dl, device)\n","  _, y_test = next(iter(test_dl))\n","  cm = confusion_matrix(y_test, pred[0].cpu(), normalize = 'pred')\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","  disp.plot()\n","  plt.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVNARhjoMe9I","outputId":"e3f45564-da34-4ade-9543-6d3d2acb26f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape:  torch.Size([13824, 1, 22, 281])\n","y_train shape:  torch.Size([13824])\n","x_validation shape:  torch.Size([864, 1, 22, 281])\n","y_validation shape:  torch.Size([864])\n","x_test shape:  torch.Size([864, 1, 22, 281])\n","y_test shape:  torch.Size([864])\n","\n"," training start\n","epoch: 10, training acc: 0.4117476851851852 val_acc: 0.39699074074074076\n","epoch: 20, training acc: 0.45970775462962965 val_acc: 0.45949074074074076\n","epoch: 30, training acc: 0.46997974537037035 val_acc: 0.5046296296296297\n","epoch: 40, training acc: 0.4908854166666667 val_acc: 0.45601851851851855\n","epoch: 50, training acc: 0.5056423611111112 val_acc: 0.4722222222222222\n","epoch: 60, training acc: 0.5109953703703703 val_acc: 0.49421296296296297\n","epoch: 70, training acc: 0.5214120370370371 val_acc: 0.4722222222222222\n","epoch: 80, training acc: 0.5211950231481481 val_acc: 0.4791666666666667\n","epoch: 90, training acc: 0.5343605324074074 val_acc: 0.4756944444444444\n","epoch: 100, training acc: 0.5410879629629629 val_acc: 0.4710648148148148\n","epoch: 110, training acc: 0.5456452546296297 val_acc: 0.4548611111111111\n","epoch: 120, training acc: 0.5498408564814815 val_acc: 0.47453703703703703\n","epoch: 130, training acc: 0.5584490740740741 val_acc: 0.48842592592592593\n","epoch: 140, training acc: 0.5570746527777778 val_acc: 0.4826388888888889\n","epoch: 150, training acc: 0.5669126157407407 val_acc: 0.49421296296296297\n"]}],"source":["if __name__ ==\"__main__\":\n","  main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaIuysRdjs7V"},"outputs":[],"source":["def val_model_CCNN(model, test_dl, device):\n","  model.eval()\n","  # print('test start')\n","  acc = 0\n","  with torch.no_grad():\n","    for x_test, y_test in test_dl:\n","      x_test, y_test = x_test.to(device), y_test.to(device)\n","      pred, feature_pred = model(x_test)\n","      pred = torch.argmax(pred, axis = 1)\n","      acc += (pred==y_test).sum().item()\n","  acc /= len(test_dl.dataset)\n"," \n","  return acc\n","\n","## if using MLP to fusion\n","def train_fusion_model(model, train_dl, test_dl, device, config):\n","  # set the optimizer and corresponding loss function\n","  optimizer = getattr(optim, config['optimizer'])(model[\"fusion model\"].parameters(), lr=config['lr'], weight_decay=0.0001)\n","  if config['model'] == 'MCNN':\n","    criterion = nn.CrossEntropyLoss()\n","  elif config['model'] == 'CCNN':\n","    criterion = nn.MSELoss()\n","  #lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n","  record = {\n","      'train_loss': [],\n","      'train_acc': [],\n","      'val_loss': [],\n","      'val_acc': []\n","      }\n","  \n","  max_acc = 0\n","  print('\\n training start')\n","  # start training process\n","  for epoch in range(config['epoch']):\n","    model[\"fusion model\"].train()\n","    train_loss = 0\n","    train_acc = 0\n","    loss = 0\n","    print(\"epoch: \", epoch)\n","    for x_train, y_train in train_dl:\n","      x_train, y_train = x_train.to(device), y_train.to(device)\n","      optimizer.zero_grad()\n","      latent1, _ = model['CNN1'](x_train) # model prediction\n","      latent2, _ = model['CNN2'](x_train)\n","      latent3, _ = model['CNN3'](x_train)\n","      latent4, _ = model['CNN4'](x_train)\n","      feat_vec = torch.cat([latent1, latent2, latent3, latent4], 1)\n","      _, out = model['fusion model'](feat_vec)\n","      if config['model'] == 'MCNN':\n","        loss = criterion(out, y_train) # calculate loss\n","        pred = torch.argmax(out, axis = 1) # max out the prediction\n","      elif config['model'] == 'CCNN':\n","        loss = criterion(out, feat_vec) # calculate loss\n","        pred = torch.argmax(_, axis = 1) # max out the prediction\n","      train_loss += loss.detach().cpu().item()\n","      train_acc += (pred==y_train).sum().item() # count the corrected classified casese\n","\n","      loss.backward() # derive backpropagation\n","      optimizer.step() # update model\n","\n","    for x_test, y_test in test_dl:\n","      x_test, y_test = x_test.to(device), y_test.to(device)\n","      latent1, _ = model['CNN1'](x_test) # model prediction\n","      latent2, _ = model['CNN2'](x_test)\n","      latent3, _ = model['CNN3'](x_test)\n","      latent4, _ = model['CNN4'](x_test)\n","      feat_vec_val = torch.cat([latent1, latent2, latent3, latent4], 1)\n","      val_dataset = TensorDataset(feat_vec_val, y_test)\n","    \n","    del latent1, latent2, latent3, latent4, feat_vec_val\n","\n","    val_dl = DataLoader(\n","        dataset = val_dataset,\n","        batch_size = len(val_dataset),\n","        shuffle = False,\n","        num_workers = 0   \n","    )\n","\n","    # save the model which produce maximum validation accuracy\n","    if config['model'] == 'MCNN':\n","      val_acc = val_model(model['fusion model'], val_dl, device)\n","    elif config['model'] == 'CCNN':\n","      val_acc = val_model_CCNN(model['fusion model'], val_dl, device)\n","    \n","    ## free tensor dataset\n","    del val_dataset, val_dl\n","    torch.cuda.empty_cache()\n","\n","    if(val_acc>max_acc):\n","      max_acc = val_acc\n","      torch.save(model['fusion model'].state_dict(), f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")\n","\n","    record['train_loss'].append(train_loss/len(train_dl))\n","    record['train_acc'].append(train_acc/len(train_dl.dataset))\n","    record['val_acc'].append(val_acc)\n","      \n","    #lr_scheduler.step()\n","    if (epoch+1)%10 == 0:\n","      print(f'epoch: {epoch+1}, training acc: {train_acc/len(train_dl.dataset)} val_acc: {val_acc}')\n","  \n","  return record\n"]},{"cell_type":"markdown","source":["**For training fusion model**"],"metadata":{"id":"ISzjoxMMbFZE"}},{"cell_type":"code","source":["def val_model(model, test_dl, device):\n","  model['fusion model'].eval()\n","  # print('test start')\n","  acc = 0\n","  with torch.no_grad():\n","        for x_test, y_test in test_dl:\n","            x_test, y_test = x_test.to(device), y_test.to(device)\n","            latent1, _ = model['CNN1'](x_test) # model prediction\n","            latent2, _ = model['CNN2'](x_test)\n","            latent3, _ = model['CNN3'](x_test)\n","            latent4, _ = model['CNN4'](x_test)\n","            feat_vec = torch.cat([latent1, latent2, latent3, latent4], 1)\n","            _, pred= model['fusion model'](feat_vec)\n","            pred = pred.mean(0)\n","            pred = torch.argmax(pred)\n","            acc += (pred==y_test[0]).item()\n","\n","  acc /= len(test_dl.dataset)/18\n"," \n","  return acc\n","\n","## if using MLP to fusion\n","def train_fusion_model(model, train_dl, test_dl, device, config):\n","  # set the optimizer and corresponding loss function\n","  optimizer = getattr(optim, config['optimizer'])(model[\"fusion model\"].parameters(), lr=config['lr'], weight_decay=0.0001)\n","  if config['model'] == 'MCNN':\n","    criterion = nn.CrossEntropyLoss()\n","  elif config['model'] == 'CCNN':\n","    criterion = nn.MSELoss()\n","  \n","  model['CNN1'].eval()\n","  model['CNN2'].eval()\n","  model['CNN3'].eval()\n","  model['CNN4'].eval()\n","\n","  record = {\n","      'train_loss': [],\n","      'train_acc': [],\n","      'val_loss': [],\n","      'val_acc': []\n","      }\n","  \n","  max_acc = 0\n","  print('\\n training start')\n","  # start training process\n","  for epoch in range(config['epoch']):\n","    model[\"fusion model\"].train()\n","    train_loss = 0\n","    train_acc = 0\n","    loss = 0\n","    print(\"epoch: \", epoch)\n","    for x_train, y_train in train_dl:\n","      x_train, y_train = x_train.to(device), y_train.to(device)\n","      optimizer.zero_grad()\n","      with torch.no_grad():\n","        latent1, _ = model['CNN1'](x_train) # model prediction\n","        latent2, _ = model['CNN2'](x_train)\n","        latent3, _ = model['CNN3'](x_train)\n","        latent4, _ = model['CNN4'](x_train)\n","        feat_vec = torch.cat([latent1, latent2, latent3, latent4], 1)\n","      _, out = model['fusion model'](feat_vec)\n","      if config['model'] == 'MCNN':\n","        loss = criterion(out, y_train) # calculate loss\n","        pred = torch.argmax(out, axis = 1) # max out the prediction\n","      elif config['model'] == 'CCNN':\n","        loss = criterion(out, feat_vec) # calculate loss\n","        pred = torch.argmax(_, axis = 1) # max out the prediction\n","      train_loss += loss.detach().cpu().item()\n","      train_acc += (pred==y_train).sum().item() # count the corrected classified casese\n","\n","      loss.backward() # derive backpropagation\n","      optimizer.step() # update model\n","    \n","    del latent1, latent2, latent3, latent4, feat_vec, out\n","\n","    val_acc = val_model(model, test_dl, device)\n","    \n","    ## free tensor dataset\n","    torch.cuda.empty_cache()\n","\n","    if(val_acc>max_acc):\n","      max_acc = val_acc\n","      torch.save(model['fusion model'].state_dict(), f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}_test16\")\n","\n","    record['train_loss'].append(train_loss/len(train_dl))\n","    record['train_acc'].append(train_acc/len(train_dl.dataset))\n","    record['val_acc'].append(val_acc)\n","      \n","    #lr_scheduler.step()\n","    if (epoch+1)%10 == 0:\n","      print(f'epoch: {epoch+1}, training acc: {train_acc/len(train_dl.dataset)} val_acc: {val_acc}')\n","  \n","  return record\n"],"metadata":{"id":"MnCp1qX6a5QP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DuCr8aIb9N46"},"outputs":[],"source":["def train_fusion_model_test(model, train_dl, test_dl, device, config):\n","  # set the optimizer and corresponding loss function\n","  optimizer = getattr(optim, config['optimizer'])(model[\"fusion model\"].parameters(), lr=config['lr'], weight_decay=0.0001)\n","  if config['model'] == 'MCNN':\n","    criterion = nn.CrossEntropyLoss()\n","  elif config['model'] == 'CCNN':\n","    criterion = nn.MSELoss()\n","    #train_dl.batch = 1\n","  #lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n","  record = {\n","      'train_loss': [],\n","      'train_acc': [],\n","      'val_loss': [],\n","      'val_acc': []\n","      }\n","  \n","  max_acc = 0\n","  print('\\n training start')\n","  # start training process\n","  for epoch in range(config['epoch']):\n","    model[\"fusion model\"].train()\n","    train_loss = 0\n","    train_acc = 0\n","    loss = 0\n","    print(\"epoch: \", epoch)\n","    for x_train, y_train in train_dl:\n","      x_train, y_train = x_train.to(device), y_train.to(device)\n","      optimizer.zero_grad()\n","      latent1, _ = model['CNN1'](x_train) # model prediction\n","      latent2, _ = model['CNN2'](x_train)\n","      latent3, _ = model['CNN3'](x_train)\n","      latent4, _ = model['CNN4'](x_train)\n","      feat_vec = torch.cat([latent1, latent2, latent3, latent4], 1)\n","      _, out = model['fusion model'](feat_vec)\n","      if config['model'] == 'MCNN':\n","        loss = criterion(out, y_train) # calculate loss\n","        pred = torch.argmax(out, axis = 1) # max out the prediction\n","      elif config['model'] == 'CCNN':\n","        loss = criterion(out, feat_vec) # calculate loss\n","        pred = torch.argmax(_, axis = 1) # max out the prediction\n","      train_loss += loss.detach().cpu().item()\n","      train_acc += (pred==y_train).sum().item() # count the corrected classified casese\n","\n","      loss.backward() # derive backpropagation\n","      optimizer.step() # update model\n","\n","    '''# validation\n","    for x_test, y_test in test_dl:\n","      x_test, y_test = x_test.to(device), y_test.to(device)\n","      latent1, _ = model['CNN1'](x_test) # model prediction\n","      latent2, _ = model['CNN2'](x_test)\n","      latent3, _ = model['CNN3'](x_test)\n","      latent4, _ = model['CNN4'](x_test)\n","      feat_vec_val = torch.cat([latent1, latent2, latent3, latent4], 1)\n","      val_dataset = TensorDataset(feat_vec_val, y_test)\n","    \n","    del latent1, latent2, latent3, latent4, feat_vec_val\n","\n","    val_dl = DataLoader(\n","        dataset = val_dataset,\n","        batch_size = len(val_dataset),\n","        shuffle = False,\n","        num_workers = 0   \n","    )\n","\n","    # save the model which produce maximum validation accuracy\n","    if config['model'] == 'MCNN':\n","      val_acc = val_model(model['fusion model'], val_dl, device)\n","    elif config['model'] == 'CCNN':\n","      val_acc = val_model_CCNN(model['fusion model'], val_dl, device)\n","    \n","    ## free tensor dataset\n","    del val_dataset, val_dl\n","    torch.cuda.empty_cache()\n","\n","    if(val_acc>max_acc):\n","      max_acc = val_acc\n","      torch.save(model['fusion model'].state_dict(), f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")'''\n","\n","    record['train_loss'].append(train_loss/len(train_dl))\n","    record['train_acc'].append(train_acc/len(train_dl.dataset))\n","    #record['val_acc'].append(val_acc)\n","    if train_acc > max_acc:\n","      torch.save(model['fusion model'].state_dict(), f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")\n","      max_acc = train_acc\n","    \n","    #lr_scheduler.step()\n","    if (epoch+1)%10 == 0:\n","      print(f'epoch: {epoch+1}, training loss: {train_loss/len(train_dl)} training acc: {train_acc/len(train_dl.dataset)}')\n","  \n","  return record\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eGnsH6WDlc5c"},"outputs":[],"source":["# 如果要使用4個train好的CNN model可以用下面當作範例\n","device = get_device()\n","set_seed(33)\n","scheme = 'independent' # individual, dependent, independent, fine-tune\n","file_dir = '/content/gdrive/Shareddrives/BCI/BCI_data'\n","dl = get_dataloader(file_dir, scheme)\n","train_dl, test_dl = dl\n","config = {\n","        'epoch': 500,\n","        'optimizer': 'Adam',\n","        'lr': 0.0001,\n","        'scheme': scheme,\n","        'model': 'CCNN',\n","        'save_path': '/content/gdrive/Shareddrives/BCI/CNN_model/'\n","     }\n","CNN1 = CNN1().to(device)\n","CNN1.load_state_dict(torch.load(f\"/content/gdrive/Shareddrives/BCI/CNN_model/CNN1_independent_0.0001_128\", map_location=torch.device(device)))\n","CNN2 = CNN2().to(device)\n","CNN2.load_state_dict(torch.load(f\"/content/gdrive/Shareddrives/BCI/CNN_model/CNN2_independent_0.0001_128\", map_location=torch.device(device)))\n","CNN3 = CNN3().to(device)\n","CNN3.load_state_dict(torch.load(f\"/content/gdrive/Shareddrives/BCI/CNN_model/CNN3_independent_0.0001_128\", map_location=torch.device(device)))\n","CNN4 = CNN4().to(device)\n","CNN4.load_state_dict(torch.load(f\"/content/gdrive/Shareddrives/BCI/CNN_model/CNN4_independent_0.0001_128\", map_location=torch.device(device)))\n","fusion_model = AE().to(device)\n","model = {\"CNN1\": CNN1,\n","      \"CNN2\": CNN2,\n","      \"CNN3\": CNN3,\n","      \"CNN4\": CNN4,\n","      \"fusion model\":fusion_model}\n","\n","record = train_fusion_model_test(model, train_dl, test_dl, device, config)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uo1Fu2Qbgc_W"},"source":["Window Slide"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iB1EHbCtggHZ"},"outputs":[],"source":["# Dataloader\n","# 依據不同的scheme組成dataset\n","sliding_timestep = 1\n","def window_slide(x, y):\n","  length = x.size()[3]\n","  new_length = int(length/2)\n","  stride = int(sliding_timestep * (length/4))\n","  x_new = []\n","  y_new = []\n","  for i in range(0, length-new_length+1, stride):\n","    t_tmp = x[:, :, :, i:i+new_length]\n","    if x_new == []:\n","      x_new = t_tmp\n","      y_new = y\n","    else:\n","      x_new = torch.cat((x_new, t_tmp), 0)\n","      y_new = torch.cat((y_new, y))      \n","  return x_new, y_new\n","\n","def choose_scheme(path, scheme):\n","  global sliding_timestep\n","  sliding_timestep = 1 # 改成移動的步幅\n","  filedir = os.listdir(path)\n","  x_train, x_test, y_train, y_test, x_validation, y_validation = [], [], [], [], [], []\n","  x_fine_tune, y_fine_tune = [], []\n","  # 將第一個subject 1固定作為testing subject\n","  subject_t = \"BCIC_S01_T.mat\"\n","  subject_e = \"BCIC_S01_E.mat\"\n","  x_train = torch.Tensor(io.loadmat(os.path.join(path, subject_t))['x_train']).unsqueeze(1)\n","  y_train = torch.Tensor(io.loadmat(os.path.join(path, subject_t))['y_train']).view(-1).long()\n","  x_test = torch.Tensor(io.loadmat(os.path.join(path, subject_e))['x_test']).unsqueeze(1)\n","  y_test = torch.Tensor(io.loadmat(os.path.join(path, subject_e))['y_test']).view(-1).long()\n","  x_train, y_train = window_slide(x_train, y_train)\n","  x_test, y_test = window_slide(x_test, y_test)\n","  x_validation = x_test\n","  y_validation = y_test\n","  len_x = x_train.size()[0]\n","\n","  # 依序將其他subject的data讀入\n","  for filename in filedir:\n","    #print(filename)\n","    if filename in {subject_t, subject_e}:\n","      continue\n","    elif filename.endswith('E.mat'):\n","      x = torch.Tensor(io.loadmat(os.path.join(path, filename))['x_test']).unsqueeze(1)\n","      y = torch.Tensor(io.loadmat(os.path.join(path, filename))['y_test']).view(-1).long()\n","      x, y = window_slide(x, y)\n","      x_train = torch.cat([x_train, x])\n","      y_train = torch.cat([y_train, y])\n","      # if True: # condition\n","      #   x = torch.Tensor(io.loadmat(os.path.join(path, filename))['x_test']).unsqueeze(1)\n","      #   y = torch.Tensor(io.loadmat(os.path.join(path, filename))['y_test']).view(-1).long()\n","      #   x, y = window_slide(x, y)\n","      #   x_train = torch.cat([x_train, x])\n","      #   y_train = torch.cat([y_train, y])\n","      # else:\n","      #   x_val = torch.Tensor(io.loadmat(os.path.join(path, filename))['x_test']).unsqueeze(1)\n","      #   y_val = torch.Tensor(io.loadmat(os.path.join(path, filename))['y_test']).view(-1).long()\n","      #   x, y = window_slide(x_val, y_val)\n","      #   x_validation = torch.cat([x_validation, x_val])\n","      #   y_validation = torch.cat([y_validation, y_val])\n","      \n","    elif filename.endswith('T.mat'):\n","      x = torch.Tensor(io.loadmat(os.path.join(path, filename))['x_train']).unsqueeze(1)\n","      y = torch.Tensor(io.loadmat(os.path.join(path, filename))['y_train']).view(-1).long()\n","      x, y = window_slide(x, y)\n","      x_train = torch.cat([x_train, x])\n","      y_train = torch.cat([y_train, y])\n","      \n","\n","  # choose real training and testing data based on scheme\n","  if scheme == 'individual': # indiviual will access only training session from subject1\n","    return [x_train[:len_x], y_train[:len_x], x_test, y_test, x_validation, y_validation]\n","  elif scheme == 'dependent': # dependent scheme collect all data except for the testing sessions from sub1\n","    return [x_train, y_train, x_test, y_test, x_validation, y_validation]\n","  elif scheme == 'independent': # independent scheme collect all data except for testing and training session from sub1\n","    return [x_train[len_x:], y_train[len_x:], x_test, y_test, x_validation, y_validation]\n","  elif scheme == 'fine-tune': # the data used on fine tune is the same as individual scheme\n","    return [x_train[:len_x], y_train[:len_x], x_test, y_test, x_validation, y_validation]\n","  else:\n","    raise ValueError('unexpected scheme, enter other scheme again')\n","  \n","def get_dataloader(data_path, scheme):\n","  data = []\n","  data = choose_scheme(data_path, scheme)\n","\n","  x_train, y_train, x_test, y_test, x_validation, y_validation = data\n","\n","  print(\"x_train shape: \", x_train.size())\n","  print(\"y_train shape: \", y_train.size())\n","  print(\"x_validation shape: \", x_validation.size())\n","  print(\"y_validation shape: \", y_validation.size())\n","  print(\"x_test shape: \", x_test.size())\n","  print(\"y_test shape: \", y_test.size())\n","\n","\n","  # 存成tensordataset\n","  train_dataset = TensorDataset(x_train, y_train)\n","  test_dataset = TensorDataset(x_test, y_test)\n","  validation_dataset = TensorDataset(x_validation, y_validation)\n","  # 包成dataloader\n","  train_dl = DataLoader(\n","      dataset = train_dataset,\n","      batch_size = BATCH_SIZE,\n","      shuffle = True,\n","      num_workers = 0   \n","  )\n","  test_dl = DataLoader(\n","      dataset = test_dataset,\n","      batch_size = len(test_dataset),\n","      shuffle = False,\n","      num_workers = 0   \n","  )\n","  validation_dl = DataLoader(\n","      dataset = validation_dataset,\n","      batch_size = BATCH_SIZE,\n","      shuffle = False,\n","      num_workers = 0   \n","  )\n","  return [train_dl, test_dl, validation_dl]"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CNN_model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
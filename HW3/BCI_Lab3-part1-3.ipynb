{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BCI_Lab3-part1-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"id":"aP8ZKvAowBQW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653051352867,"user_tz":-480,"elapsed":18302,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}},"outputId":"247b55c5-ec96-4bd4-912f-c0cc24439cb4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/gdrive/Shareddrives/BCI/\")\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sIyWDdfGi3km","executionInfo":{"status":"ok","timestamp":1653051580762,"user_tz":-480,"elapsed":230,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}},"outputId":"7cf594ce-195e-4099-e010-10da228b462d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":[" BCI_data\n"," BCI_Lab3.ipynb\n"," BCI_Lab3-part1-3.ipynb\n"," CNN_model\n"," CNN_model.ipynb\n","'Deep Learning for EEG motor imagery classification based on multi-layer CNNs feature fusion.pdf'\n"," Lab3_Group5.docx\n"," model\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import random\n","from scipy import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","BATCH_SIZE = 32"],"metadata":{"id":"VFs6U92LoGfk","executionInfo":{"status":"ok","timestamp":1653051356675,"user_tz":-480,"elapsed":3810,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def get_device():\n","    return 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)"],"metadata":{"id":"Uq2T75bMn3Xi","executionInfo":{"status":"ok","timestamp":1653051356676,"user_tz":-480,"elapsed":4,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"f8fWlmZBMKan","executionInfo":{"status":"ok","timestamp":1653051356676,"user_tz":-480,"elapsed":4,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"outputs":[],"source":["class EEGNet(nn.Module):\n","    def __init__(self):\n","        super(EEGNet, self).__init__()\n","\n","        self.F1 = 8\n","        self.F2 = 16\n","        self.D = 2\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(1, self.F1, (1, 64), padding=(0, 32), bias=False),\n","            nn.BatchNorm2d(self.F1)\n","        )\n","\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(self.F1, self.D*self.F1, (22, 1), groups=self.F1, bias=False),\n","            nn.BatchNorm2d(self.D*self.F1),\n","            nn.ELU(),\n","            nn.AvgPool2d((1, 4)),\n","            nn.Dropout(0.5)\n","        )\n","\n","        self.Conv3 = nn.Sequential(\n","            nn.Conv2d(self.D*self.F1, self.D*self.F1, (1, 16), padding=(0, 8), groups=self.D*self.F1, bias=False),\n","            nn.Conv2d(self.D*self.F1, self.F2, (1, 1), bias=False),\n","            nn.BatchNorm2d(self.F2),\n","            nn.ELU(),\n","            nn.AvgPool2d((1, 8)),\n","            nn.Dropout(0.5)\n","        )\n","\n","        self.classifier = nn.Linear(16*17, 4, bias=True)\n","        #self.softmax = nn.Softmax()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.Conv3(x)\n","        \n","        x = x.view(-1, 16*17)\n","        x = self.classifier(x)\n","        #x = self.softmax(x)\n","        return x\n","\n","class ShallowConvNet(nn.Module):\n","    def __init__(self):\n","        super(ShallowConvNet, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 40, (1, 13), bias=False)\n","        self.conv2 = nn.Conv2d(40, 40, (22, 1), bias=False)\n","        self.Bn1   = nn.BatchNorm2d(40)\n","        # self.SquareLayer = square_layer()\n","        self.AvgPool1 = nn.AvgPool2d((1, 35), stride=(1, 7))\n","        # self.LogLayer = Log_layer()\n","        self.Drop1 = nn.Dropout(0.25)\n","        self.classifier = nn.Linear(40*74, 4, bias=True)\n","        #self.softmax = nn.Softmax()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.Bn1(x)\n","        x = x ** 2\n","        x = self.AvgPool1(x)\n","        x = torch.log(x)\n","        x = self.Drop1(x)\n","        x = x.view(-1, 40*74)\n","        x = self.classifier(x)\n","\n","        #x = self.softmax(x)\n","        return x\n","\n","class SCCNet(nn.Module):\n","    def __init__(self):\n","        super(SCCNet, self).__init__()\n","        # bs, 1, channel, sample\n","        self.conv1 = nn.Conv2d(1, 22, (22, 1))\n","        self.Bn1 = nn.BatchNorm2d(22)\n","        # bs, 22, 1, sample\n","        self.conv2 = nn.Conv2d(22, 20, (1, 12), padding=(0, 6))\n","        self.Bn2   = nn.BatchNorm2d(20)\n","        # self.SquareLayer = square_layer()\n","        self.Drop1 = nn.Dropout(0.5)\n","        self.AvgPool1 = nn.AvgPool2d((1, 62), stride=(1, 12))\n","        self.classifier = nn.Linear(840, 4, bias=True)\n","        #self.softmax = nn.Softmax()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.Bn1(x)\n","        x = self.conv2(x)\n","        x = self.Bn2(x)\n","        x = x ** 2\n","        x = self.Drop1(x)\n","        x = self.AvgPool1(x)\n","        x = torch.log(x)\n","        x = x.view(-1, 840)\n","        x = self.classifier(x)\n","\n","        #x = self.softmax(x)\n","        return x"]},{"cell_type":"code","source":["# Dataloader\n","def choose_scheme(path, scheme):\n","  filedir = os.listdir(path)\n","  x_train, x_test, y_train, y_test = [], [], [], []\n","  x_fine_tune, y_fine_tune = [], []\n","  subject_t = \"BCIC_S01_T.mat\"\n","  subject_e = \"BCIC_S01_E.mat\"\n","  x_train = torch.Tensor(io.loadmat(os.path.join(path, subject_t))['x_train']).unsqueeze(1)\n","  y_train = torch.Tensor(io.loadmat(os.path.join(path, subject_t))['y_train']).view(-1).long()\n","  x_test = torch.Tensor(io.loadmat(os.path.join(path, subject_e))['x_test']).unsqueeze(1)\n","  y_test = torch.Tensor(io.loadmat(os.path.join(path, subject_e))['y_test']).view(-1).long()\n","  len_x = x_train.size()[0]\n","\n","  # fine-tune\n","  for filename in filedir:\n","    #print(filename)\n","    if filename in {subject_t, subject_e}:\n","      continue\n","    elif filename.endswith('E.mat'):\n","      x = torch.Tensor(io.loadmat(os.path.join(path, filename))['x_test']).unsqueeze(1)\n","      y = torch.Tensor(io.loadmat(os.path.join(path, filename))['y_test']).view(-1).long()\n","      x_train = torch.cat([x_train, x])\n","      y_train = torch.cat([y_train, y])\n","    elif filename.endswith('T.mat'):\n","      x = torch.Tensor(io.loadmat(os.path.join(path, filename))['x_train']).unsqueeze(1)\n","      y = torch.Tensor(io.loadmat(os.path.join(path, filename))['y_train']).view(-1).long()\n","      x_train = torch.cat([x_train, x])\n","      y_train = torch.cat([y_train, y])\n","\n","  # choose real training and testing data based on scheme\n","  if scheme == 'individual':\n","    return [x_train[:len_x], y_train[:len_x], x_test, y_test]\n","  elif scheme == 'dependent':\n","    return [x_train, y_train, x_test, y_test]\n","  elif scheme == 'independent':\n","    return [x_train[len_x:], y_train[len_x:], x_test, y_test]\n","  elif scheme == 'fine-tune':\n","    return [x_train[:len_x], y_train[:len_x], x_test, y_test]\n","  else:\n","    raise ValueError('unexpected scheme, enter other scheme again')\n","  \n","def get_dataloader(data_path, scheme):\n","  data = []\n","  data = choose_scheme(data_path, scheme)\n","\n","  x_train, y_train, x_test, y_test = data\n","\n","  print(\"x_train shape: \", x_train.size())\n","  print(\"y_train shape: \", y_train.size())\n","  print(\"x_test shape: \", x_test.size())\n","  print(\"y_test shape: \", y_test.size())\n","\n","  # 存成tensordataset\n","  train_dataset = TensorDataset(x_train, y_train)\n","  test_dataset = TensorDataset(x_test, y_test)\n","  # 包成dataloader\n","  train_dl = DataLoader(\n","      dataset = train_dataset,\n","      batch_size = BATCH_SIZE,\n","      shuffle = True,\n","      num_workers = 0   \n","  )\n","  test_dl = DataLoader(\n","      dataset = test_dataset,\n","      batch_size = len(test_dataset),\n","      shuffle = False,\n","      num_workers = 0   \n","  )\n","  return [train_dl, test_dl]"],"metadata":{"id":"h0apIeiNOemh","executionInfo":{"status":"ok","timestamp":1653051357100,"user_tz":-480,"elapsed":427,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_dl, test_dl, device, config):\n","  optimizer = getattr(optim, config['optimizer'])(model.parameters(), lr=config['lr'])\n","  criterion = nn.CrossEntropyLoss()\n","  #lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n","  record = {\n","      'train_loss': [],\n","      'train_acc': [],\n","      'val_loss': [],\n","      'val_acc': []\n","      }\n","  \n","  max_acc = 0\n","  print('\\n training start')\n","  model.train()\n","  for epoch in range(config['epoch']):\n","    train_loss = 0\n","    train_acc = 0\n","    for x_train, y_train in train_dl:\n","      x_train, y_train = x_train.to(device), y_train.to(device)\n","      optimizer.zero_grad()\n","      out = model(x_train)\n","      loss = criterion(out, y_train)\n","      pred = torch.argmax(out, axis = 1)\n","      train_loss += loss.detach().cpu().item()\n","      train_acc += (pred==y_train).sum().item()\n","\n","      loss.backward()\n","      optimizer.step()\n","    \n","    val_acc = val_model(model, test_dl, device)\n","    if(val_acc>max_acc):\n","      max_acc = val_acc\n","      torch.save(model.state_dict(), f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")\n","\n","    record['train_loss'].append(train_loss/len(train_dl))\n","    record['train_acc'].append(train_acc/len(train_dl.dataset))\n","    record['val_acc'].append(val_acc)\n","      \n","    #lr_scheduler.step()\n","    if (epoch+1)%10 == 0:\n","      print(f'epoch: {epoch+1}, training acc: {train_acc/len(train_dl.dataset)} val_acc: {val_acc}')\n","  \n","  return record\n","\n","def val_model(model, test_dl, device):\n","  model.eval()\n","  # print('test start')\n","  acc = 0\n","  with torch.no_grad():\n","    for x_test, y_test in test_dl:\n","      x_test, y_test = x_test.to(device), y_test.to(device)\n","      pred = model(x_test)\n","      pred = torch.argmax(pred, axis = 1)\n","      acc += (pred==y_test).sum().item()\n","  acc /= len(test_dl.dataset)\n"," \n","  return acc\n","\n","def test_model(model, test_dl, device):\n","  model.eval()\n","  # print('test start')\n","  acc = 0\n","  output = []\n","  with torch.no_grad():\n","    for x_test, y_test in test_dl:\n","      x_test, y_test = x_test.to(device), y_test.to(device)\n","      pred = model(x_test)\n","      pred = torch.argmax(pred, axis = 1)\n","      output.append(pred)\n","      acc += (pred==y_test).sum().item()\n","  acc /= len(test_dl.dataset)\n","  print(f\"testing accuracy: {acc*100}%\")\n"," \n","  return output"],"metadata":{"id":"PHFL5_A9qrC_","executionInfo":{"status":"ok","timestamp":1653051357101,"user_tz":-480,"elapsed":3,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def plot_confusion_matrix(model, test_dl, device):\n","  pred = test_model(model, test_dl, device)\n","  _, y_test = next(iter(test_dl))\n","  cm = confusion_matrix(y_test, pred[0].cpu(), normalize = 'pred')\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","  disp.plot()\n","  plt.plot()"],"metadata":{"id":"uFf2WRdSoP1N","executionInfo":{"status":"ok","timestamp":1653051357101,"user_tz":-480,"elapsed":3,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# A & B."],"metadata":{"id":"yx8Xa8DzosP-"}},{"cell_type":"markdown","source":["## Subject-Independent"],"metadata":{"id":"GeutdNu5-4ep"}},{"cell_type":"code","source":["device = get_device()\n","set_seed(33)\n","file_dir = '/content/gdrive/Shareddrives/BCI/BCI_data'\n","scheme = 'independent' # individual, dependent, independent, fine-tune\n","dl = get_dataloader(file_dir, scheme)\n","\n","if scheme == 'fine-tune':\n","  tune_dl, test_dl = dl  \n","  config = {\n","      'epoch': 200,\n","      'optimizer': 'Adam',\n","      'lr': 0.0001,\n","      'model': 'EEGNet',\n","      'scheme': scheme,\n","      'save_path': '/content/gdrive/Shareddrives/BCI/model/'\n","  }\n","  # os.makedirs('/content/gdrive/MyDrive/model', exist_ok=True)\n","  model = EEGNet().to(device)\n","  model.load_state_dict(torch.load('/content/gdrive/Shareddrives/BCI/model/EEGNet_independent_0.001_32')) # 要改load model的檔名\n","  #for name, param in model.named_parameters():\n","  #  if name != 'classifier.weight' and name != 'classifier.bias':\n","  #    param.requires_grad = False\n","  loss_record = train_model(model, tune_dl, test_dl, device, config)\n","else:\n","  train_dl, test_dl = dl\n","  model = EEGNet().to(device)\n","  config = {\n","      'epoch': 100,\n","      'optimizer': 'Adam',\n","      'lr': 0.001,\n","      'scheme': scheme,\n","      'model': 'EEGNet',\n","      'save_path': '/content/gdrive/Shareddrives/BCI/model/'\n","  }\n","  # os.makedirs('/content/gdrive/MyDrive/model', exist_ok=True)\n","  loss_record = train_model(model, train_dl, test_dl, device, config)\n","\n","# plot confusion matrix\n","model = EEGNet().to(device)\n","model.load_state_dict(torch.load(f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")) # 要改load model的檔名\n","plot_confusion_matrix(model, test_dl, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"yg3rDMHn6qpC","executionInfo":{"status":"error","timestamp":1653051357840,"user_tz":-480,"elapsed":741,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}},"outputId":"5e20e2c8-62f9-4fbb-93dd-e9aecf2fb185"},"execution_count":8,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-0d418fab0ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/Shareddrives/BCI/BCI_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'independent'\u001b[0m \u001b[0;31m# individual, dependent, independent, fine-tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fine-tune'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-10e2797f7e61>\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(data_path, scheme)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-10e2797f7e61>\u001b[0m in \u001b[0;36mchoose_scheme\u001b[0;34m(path, scheme)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchoose_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mfiledir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mx_fine_tune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fine_tune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/Shareddrives/BCI/BCI_data'"]}]},{"cell_type":"markdown","source":["## Subject-Dependent"],"metadata":{"id":"oIPx1ybg-_BT"}},{"cell_type":"code","source":["device = get_device()\n","set_seed(33)\n","file_dir = '/content/gdrive/Shareddrives/BCI/BCI_data'\n","scheme = 'dependent' # individual, dependent, independent, fine-tune\n","dl = get_dataloader(file_dir, scheme)\n","\n","if scheme == 'fine-tune':\n","  tune_dl, test_dl = dl  \n","  config = {\n","      'epoch': 200,\n","      'optimizer': 'Adam',\n","      'lr': 0.0001,\n","      'model': 'EEGNet',\n","      'scheme': scheme,\n","      'save_path': '/content/gdrive/Shareddrives/BCI/model/'\n","  }\n","  # os.makedirs('/content/gdrive/MyDrive/model', exist_ok=True)\n","  model = EEGNet().to(device)\n","  model.load_state_dict(torch.load('/content/gdrive/Shareddrives/BCI/model/EEGNet_independent_0.001_32')) # 要改load model的檔名\n","  #for name, param in model.named_parameters():\n","  #  if name != 'classifier.weight' and name != 'classifier.bias':\n","  #    param.requires_grad = False\n","  loss_record = train_model(model, tune_dl, test_dl, device, config)\n","else:\n","  train_dl, test_dl = dl\n","  model = EEGNet().to(device)\n","  config = {\n","      'epoch': 100,\n","      'optimizer': 'Adam',\n","      'lr': 0.001,\n","      'scheme': scheme,\n","      'model': 'EEGNet',\n","      'save_path': '/content/gdrive/Shareddrives/BCI/model/'\n","  }\n","  # os.makedirs('/content/gdrive/MyDrive/model', exist_ok=True)\n","  loss_record = train_model(model, train_dl, test_dl, device, config)\n","\n","# plot confusion matrix\n","model = EEGNet().to(device)\n","model.load_state_dict(torch.load(f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")) # 要改load model的檔名\n","plot_confusion_matrix(model, test_dl, device)"],"metadata":{"id":"ZonI6dQC_DHJ","executionInfo":{"status":"aborted","timestamp":1653051357837,"user_tz":-480,"elapsed":6,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *Subject-indepentent + Fine-funing*"],"metadata":{"id":"E-N4ltEFBGuv"}},{"cell_type":"code","source":["device = get_device()\n","set_seed(33)\n","file_dir = '/content/gdrive/Shareddrives/BCI/BCI_data'\n","scheme = 'fine-tune' # individual, dependent, independent, fine-tune\n","dl = get_dataloader(file_dir, scheme)\n","\n","if scheme == 'fine-tune':\n","  tune_dl, test_dl = dl  \n","  config = {\n","      'epoch': 200,\n","      'optimizer': 'Adam',\n","      'lr': 0.0001,\n","      'model': 'EEGNet',\n","      'scheme': scheme,\n","      'save_path': '/content/gdrive/Shareddrives/BCI/model/'\n","  }\n","  # os.makedirs('/content/gdrive/MyDrive/model', exist_ok=True)\n","  model = EEGNet().to(device)\n","  model.load_state_dict(torch.load('/content/gdrive/Shareddrives/BCI/model/EEGNet_independent_0.001_32')) # 要改load model的檔名\n","  #for name, param in model.named_parameters():\n","  #  if name != 'classifier.weight' and name != 'classifier.bias':\n","  #    param.requires_grad = False\n","  loss_record = train_model(model, tune_dl, test_dl, device, config)\n","else:\n","  train_dl, test_dl = dl\n","  model = EEGNet().to(device)\n","  config = {\n","      'epoch': 100,\n","      'optimizer': 'Adam',\n","      'lr': 0.001,\n","      'scheme': scheme,\n","      'model': 'EEGNet',\n","      'save_path': '/content/gdrive/Shareddrives/BCI/model/'\n","  }\n","  # os.makedirs('/content/gdrive/MyDrive/model', exist_ok=True)\n","  loss_record = train_model(model, train_dl, test_dl, device, config)\n","\n","# plot confusion matrix\n","model = EEGNet().to(device)\n","model.load_state_dict(torch.load(f\"{config['save_path']+config['model']}_{config['scheme']}_{config['lr']}_{BATCH_SIZE}\")) # 要改load model的檔名\n","plot_confusion_matrix(model, test_dl, device)"],"metadata":{"id":"wJXl6lep0xY8","executionInfo":{"status":"aborted","timestamp":1653051357838,"user_tz":-480,"elapsed":7,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# C. Topographic maps"],"metadata":{"id":"dzceAHmeoBE1"}},{"cell_type":"code","source":["!pip install mne"],"metadata":{"id":"GpsvegwhbIv_","executionInfo":{"status":"aborted","timestamp":1653051357838,"user_tz":-480,"elapsed":7,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import random\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","import mne"],"metadata":{"id":"cNIv8VVVoJll","executionInfo":{"status":"aborted","timestamp":1653051357839,"user_tz":-480,"elapsed":8,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["biosemi_montage = mne.channels.make_standard_montage('biosemi64')\n","n_channels = len(biosemi_montage.ch_names)\n","fake_info = mne.create_info(ch_names=biosemi_montage.ch_names, sfreq=250.,\n","                            ch_types='eeg')\n","\n","rng = np.random.RandomState(0)\n","data = rng.normal(size=(n_channels, 1)) * 1e-6\n","fake_evoked = mne.EvokedArray(data, fake_info)\n","fake_evoked.set_montage(biosemi_montage)"],"metadata":{"id":"tKWti5p6oMIz","executionInfo":{"status":"aborted","timestamp":1653051357839,"user_tz":-480,"elapsed":7,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chs = ['Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'P1', 'Pz', 'P2', 'POz']\n","\n","montage_head = fake_evoked.get_montage()\n","ch_pos = montage_head.get_positions()['ch_pos']\n","pos = np.stack([ch_pos[ch] for ch in chs])\n"],"metadata":{"id":"UwzHdnYGoNdt","executionInfo":{"status":"aborted","timestamp":1653051357839,"user_tz":-480,"elapsed":7,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = SCCNet()\n","model.load_state_dict(torch.load('/content/gdrive/Shareddrives/BCI/model/SCCNet_independent_0.001_128'))\n","print(model)\n","\n","kernels = model.conv1.weight"],"metadata":{"id":"y3rLP6jKoQjx","executionInfo":{"status":"aborted","timestamp":1653051357840,"user_tz":-480,"elapsed":8,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(15,10))\n","for i in range(22):\n","  w = kernels[i].detach().numpy()\n","  w = np.reshape(w, -1)\n","\n","  plt.subplot(5, 5, i+1)\n","  plt.title('Kernel %d'%(i+1))\n","  mne.viz.plot_topomap(w, pos[:, :2], show = False)"],"metadata":{"id":"cx-WtrlZoRpw","executionInfo":{"status":"aborted","timestamp":1653051357840,"user_tz":-480,"elapsed":8,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2f8fXH3No6qM","executionInfo":{"status":"aborted","timestamp":1653051357840,"user_tz":-480,"elapsed":8,"user":{"displayName":"李庭逸","userId":"17692844335573755132"}}},"execution_count":null,"outputs":[]}]}